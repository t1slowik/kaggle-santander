{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Santander competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general & data analysis imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=pd.read_csv('train.csv')\n",
    "test_dataset=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dependent variable from train set to have the same structure as test set\n",
    "df_target=train_dataset['target'].copy()\n",
    "df_train=train_dataset.drop(['ID_code','target'], axis=1)\n",
    "df_test=test_dataset.drop('ID_code', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm = SMOTE(random_state=1)\n",
    "#X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "#X_val, y_val = sm.fit_resample(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training, tuning and evaluation\n",
    "random hyperparameters search, without kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(df_train.values,df_target.values,test_size=0.15,random_state=1, shuffle=True)\n",
    "X_test=df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbm parameters values for random search\n",
    "param_grid = dict(\n",
    "         objective =  ['binary'],\n",
    "         learning_rate = np.logspace(-3, -1, num=50, base=10.0),\n",
    "         feature_fraction = np.logspace(-2, -1, num=50, base=10.0),\n",
    "         num_leaves = np.arange(10,30,2),\n",
    "         min_data_in_leaf = np.arange(30,150,50),\n",
    "         bagging_fraction = np.arange(0.3,0.95,0.01),\n",
    "         bagging_freq = np.arange(3, 30, 5),\n",
    "         max_depth = [-1],\n",
    "         boosting_type = ['gbdt'],\n",
    "         metric = ['auc'],\n",
    "         min_sum_hessian_in_leaf = np.logspace(-4, 2, num=50, base=10.0),\n",
    "         n_jobs = [-1],\n",
    "         num_round = [2500]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(param_grid, X_train, X_val, y_train, y_val, iterations):\n",
    "    train_set = lgb.Dataset(X_train, label=y_train)\n",
    "    val_set = lgb.Dataset(X_val, label=y_val)\n",
    "    param_list=list(param_grid.keys())\n",
    "    metrics_list=['ROC_train','ROC_val','ROC_diff']\n",
    "    logging_list=param_list+metrics_list\n",
    "    results=[]\n",
    "    try:\n",
    "        for i in range(iterations):\n",
    "            print(f'iteration {i+1} of {iterations}')\n",
    "            # randomly select parameters\n",
    "            param = dict()\n",
    "            for key in param_grid:\n",
    "                param[key] = np.random.choice(param_grid[key])\n",
    "            print(f'selected params:{param}')\n",
    "            # train the model\n",
    "            clf = lgb.train(param, train_set, valid_sets=[train_set,val_set], verbose_eval=500,early_stopping_rounds = 400)\n",
    "            # calculate & log statistics\n",
    "            y_train_proba=clf.predict(X_train)\n",
    "            y_val_proba=clf.predict(X_val)\n",
    "            param['ROC_train']=roc_auc_score(y_train,y_train_proba)\n",
    "            param['ROC_val']=roc_auc_score(y_val,y_val_proba)\n",
    "            param['ROC_diff']=param['ROC_train']-param['ROC_val']\n",
    "            logging_list\n",
    "            # log results\n",
    "            result_line=[]\n",
    "            # log parameters\n",
    "            for key in logging_list:\n",
    "                result_line.append(param[key])\n",
    "            results.append(result_line)\n",
    "    except(KeyboardInterrupt):\n",
    "        pass\n",
    "    # save results to file\n",
    "    result_df=pd.DataFrame(results, columns=logging_list)\n",
    "    result_df.to_csv('hp_search.csv', index=False)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 100\n",
      "selected params:{'objective': 'binary', 'learning_rate': 0.02442053094548651, 'feature_fraction': 0.0517947467923121, 'num_leaves': 12, 'min_data_in_leaf': 30, 'bagging_fraction': 0.49000000000000016, 'bagging_freq': 23, 'max_depth': -1, 'boosting_type': 'gbdt', 'metric': 'auc', 'min_sum_hessian_in_leaf': 56.89866029018293, 'n_jobs': -1, 'num_round': 2500}\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[500]\ttraining's auc: 0.898337\tvalid_1's auc: 0.881642\n",
      "[1000]\ttraining's auc: 0.911384\tvalid_1's auc: 0.891246\n",
      "[1500]\ttraining's auc: 0.91977\tvalid_1's auc: 0.896037\n",
      "[2000]\ttraining's auc: 0.92548\tvalid_1's auc: 0.898719\n",
      "[2500]\ttraining's auc: 0.930083\tvalid_1's auc: 0.899766\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\ttraining's auc: 0.930083\tvalid_1's auc: 0.899766\n",
      "iteration 2 of 100\n",
      "selected params:{'objective': 'binary', 'learning_rate': 0.015264179671752334, 'feature_fraction': 0.09102981779915217, 'num_leaves': 16, 'min_data_in_leaf': 130, 'bagging_fraction': 0.48000000000000015, 'bagging_freq': 8, 'max_depth': -1, 'boosting_type': 'gbdt', 'metric': 'auc', 'min_sum_hessian_in_leaf': 75.43120063354607, 'n_jobs': -1, 'num_round': 2500}\n",
      "Training until validation scores don't improve for 400 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slowito1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's auc: 0.890582\tvalid_1's auc: 0.872807\n",
      "[1000]\ttraining's auc: 0.907909\tvalid_1's auc: 0.885749\n",
      "[1500]\ttraining's auc: 0.917831\tvalid_1's auc: 0.892538\n",
      "[2000]\ttraining's auc: 0.924678\tvalid_1's auc: 0.896492\n",
      "[2500]\ttraining's auc: 0.929682\tvalid_1's auc: 0.898608\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\ttraining's auc: 0.929682\tvalid_1's auc: 0.898608\n",
      "iteration 3 of 100\n",
      "selected params:{'objective': 'binary', 'learning_rate': 0.04714866363457392, 'feature_fraction': 0.1, 'num_leaves': 14, 'min_data_in_leaf': 130, 'bagging_fraction': 0.8100000000000005, 'bagging_freq': 23, 'max_depth': -1, 'boosting_type': 'gbdt', 'metric': 'auc', 'min_sum_hessian_in_leaf': 0.0016768329368110084, 'n_jobs': -1, 'num_round': 2500}\n",
      "Training until validation scores don't improve for 400 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slowito1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's auc: 0.91583\tvalid_1's auc: 0.887103\n",
      "[1000]\ttraining's auc: 0.935821\tvalid_1's auc: 0.896734\n",
      "[1500]\ttraining's auc: 0.94793\tvalid_1's auc: 0.898781\n",
      "[2000]\ttraining's auc: 0.95776\tvalid_1's auc: 0.899059\n",
      "Early stopping, best iteration is:\n",
      "[1879]\ttraining's auc: 0.955522\tvalid_1's auc: 0.899271\n",
      "iteration 4 of 100\n",
      "selected params:{'objective': 'binary', 'learning_rate': 0.012648552168552958, 'feature_fraction': 0.011513953993264475, 'num_leaves': 28, 'min_data_in_leaf': 30, 'bagging_fraction': 0.5600000000000003, 'bagging_freq': 13, 'max_depth': -1, 'boosting_type': 'gbdt', 'metric': 'auc', 'min_sum_hessian_in_leaf': 0.00013257113655901095, 'n_jobs': -1, 'num_round': 2500}\n",
      "Training until validation scores don't improve for 400 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slowito1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's auc: 0.926405\tvalid_1's auc: 0.873574\n"
     ]
    }
   ],
   "source": [
    "random_search(param_grid, X_train, X_val, y_train, y_val, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final training\n",
    "kfold with best found hyperparameters\n",
    "predicted probabilities are mean of predictions from all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating fold 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slowito1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds.\n",
      "[500]\ttraining's auc: 0.904735\tvalid_1's auc: 0.853201\n",
      "[1000]\ttraining's auc: 0.938294\tvalid_1's auc: 0.875071\n",
      "[1500]\ttraining's auc: 0.955639\tvalid_1's auc: 0.884141\n",
      "[2000]\ttraining's auc: 0.967035\tvalid_1's auc: 0.889215\n",
      "[2500]\ttraining's auc: 0.975449\tvalid_1's auc: 0.891839\n",
      "[3000]\ttraining's auc: 0.98191\tvalid_1's auc: 0.893351\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's auc: 0.98191\tvalid_1's auc: 0.893351\n",
      "Fold 1 calcutated in 663.8956127166748.\n",
      "Calculating fold 2/3...\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[500]\ttraining's auc: 0.90724\tvalid_1's auc: 0.848393\n",
      "[1000]\ttraining's auc: 0.940216\tvalid_1's auc: 0.87054\n",
      "[1500]\ttraining's auc: 0.957037\tvalid_1's auc: 0.879951\n",
      "[2000]\ttraining's auc: 0.968067\tvalid_1's auc: 0.885308\n",
      "[2500]\ttraining's auc: 0.976115\tvalid_1's auc: 0.888255\n",
      "[3000]\ttraining's auc: 0.982512\tvalid_1's auc: 0.889822\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's auc: 0.982512\tvalid_1's auc: 0.889822\n",
      "Fold 2 calcutated in 665.1898519992828.\n",
      "Calculating fold 3/3...\n",
      "Training until validation scores don't improve for 400 rounds.\n",
      "[500]\ttraining's auc: 0.905704\tvalid_1's auc: 0.847771\n",
      "[1000]\ttraining's auc: 0.939666\tvalid_1's auc: 0.871156\n",
      "[1500]\ttraining's auc: 0.956649\tvalid_1's auc: 0.881113\n",
      "[2000]\ttraining's auc: 0.967608\tvalid_1's auc: 0.886128\n",
      "[2500]\ttraining's auc: 0.975695\tvalid_1's auc: 0.888953\n",
      "[3000]\ttraining's auc: 0.982002\tvalid_1's auc: 0.890525\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's auc: 0.982002\tvalid_1's auc: 0.890525\n",
      "Fold 3 calcutated in 724.9495577812195.\n"
     ]
    }
   ],
   "source": [
    "param = {'objective': 'binary', 'metric': 'auc', 'learning_rate': 0.01, 'num_rounds': 3000, 'verbose': 1}\n",
    "        # , 'device': 'gpu', 'gpu_use_dp': False}\n",
    "\n",
    "# predicted probabilities on test set (competition set)\n",
    "y_probs = np.zeros(len(df_test.values))\n",
    "fold_n=3\n",
    "folds = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=30)\n",
    "for i, (train_index, valid_index) in enumerate(folds.split(df_train,df_target)):\n",
    "    tic=time.time()\n",
    "    print(f'Calculating fold {i+1}/{fold_n}...')\n",
    "    train_set = lgb.Dataset(df_train.iloc[train_index], label=df_target.iloc[train_index])\n",
    "    val_set = lgb.Dataset(df_train.iloc[valid_index], label=df_target.iloc[valid_index])\n",
    "    clf = lgb.train(param, train_set, valid_sets=[train_set,val_set], verbose_eval=500,early_stopping_rounds = 400)\n",
    "    y_probs += clf.predict(df_test.values, num_iteration=clf.best_iteration)/fold_n\n",
    "    toc=time.time()\n",
    "    print(f'Fold {i+1} calcutated in {toc-tic}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_val_probs=clf.predict(X_val)\n",
    "#y_val_preds=np.where(y_val_probs>0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc_auc_score(y_val, y_val_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_probs=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08730093, 0.2432966 , 0.20597767, ..., 0.00640229, 0.07279869,\n",
       "       0.05458183])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"ID_code\":test_dataset[\"ID_code\"].values})\n",
    "submission_df[\"target\"] = y_probs\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
